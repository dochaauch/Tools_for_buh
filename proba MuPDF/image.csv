page,line_id,line
1,1,Abstract There are Iwo exiuing slrulegiea {or apply»
1,2,"w: m a WW Emma. {""g Pm-lmyf“,k""'gt'j'§; W;m*;'“_“'j°""*%°d°;;-"
1,3,"|iun nludcl culled BERT. whxch \lamh I'm ””9”“ “‘5 *' ”mm"" V a“ -""""""""“""”g' e"
1,4,Bxdlmclmnml Encodcl chmscnmuans fmm fei‘mm’bﬂsed uwach- §|IC_h M ELMO ”Mm
1,5,"Tmmmmm. umm- m-cm language mm, at al.. 2018a}. "">85 laskrspeuﬁc archuecmres Hm"
1,6,mumuon models 1Pelers el 4].. 20m; Rndr include me prerlrained represenminm as addlv
1,7,"{nnl cl al. 20m. BERT is dcxlgnl‘d In W, noun] fcmum. The ﬁnc-ummg approach such 113"
1,8,mm deep bidirecnonnl mpmsemnnons from |he Generalhe Pmmmed Transformer (OpenAl
1,9,"“""‘u'w'wv‘u' ”3’1""""“W “'""""'“”""‘“¥ """" hm"" GPT) (Rudfmd =| “1,. 2018 L inuodum minimal"
1,10,"191’! and n2!“ come“ In all layers, A. n m7 . . ."
1,11,- . - |askrspeclﬁc paramelers. and IS named on me
1,12,\uIL |hc [m‘dramcd BERT nludcl can he ﬁner _ .» .
1,13,"nlned wim J|l§| one ndduional oulplll layer “0“”me ”519 by “mply ﬁne'mnmg 11"" PW"
1,14,".0 mm mumpmcﬁm models [0, a mu. muncd pumnmers, Thc two approaches sham the"
1,15,range 0mm such as quesuon mhwering and same nhjecuve funcIion during preVlraining. where
1,16,language infcmncc. wnhum suhsmnlial msk- “16)! use llnidimcllonul nguuge model. 10 learn
1,17,wedﬁc W‘hilmum I'Wdiﬁcnnom‘. general language mpmscmunons.
1,18,1
